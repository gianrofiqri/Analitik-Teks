{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "vwo2VR05wqjK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHgdJca7wqjL"
      },
      "outputs": [],
      "source": [
        "#@title Twitter Auth Token\n",
        "twitter_auth_token = 'e95d73bd6949da13c44da2b8acc3a6181584bef2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYwTrcSwwqjO"
      },
      "outputs": [],
      "source": [
        "# Mencegah Colab disconnect\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mBn2pUAXwqjQ"
      },
      "outputs": [],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nLhkn-UCwqjR"
      },
      "outputs": [],
      "source": [
        "!pip install playwright\n",
        "!playwright install-deps\n",
        "!playwright install chromium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Scraping Data Twitter"
      ],
      "metadata": {
        "id": "NzTUadyVwqjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZbW3FNswqjT"
      },
      "outputs": [],
      "source": [
        "filename = 'penggundulanhutan.csv'\n",
        "search_keyword = '\"Deforestasi\", \"Hutan Gundul\", \" since:2025-11-20 until:2025-12-20 lang:id'\n",
        "limit = 1000\n",
        "\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xmXsmM2wqjT"
      },
      "outputs": [],
      "source": [
        "# Scraping Twitter - PERBAIKAN PATH\n",
        "filename = 'penggundulanhutan.csv'\n",
        "!npx --yes tweet-harvest@latest \\\n",
        "    -o \"penggundulanhutan.csv\" \\\n",
        "    -s \"deforestasi lang:id\" \\\n",
        "    --tab \"LATEST\" \\\n",
        "    -l 1000 \\\n",
        "    --token \"{twitter_auth_token}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyOFC13rwqjU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "df = pd.read_csv(file_path, delimiter=\",\")\n",
        "print(f\"Jumlah tweet: {len(df)}\")\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocessing Data"
      ],
      "metadata": {
        "id": "ki-yB5mpwqjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvIjeWGrwqjW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "\n",
        "# Load dataset dari direktori Colab\n",
        "file_path = '/content/tweets-data/penggundulanhutan.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Data berhasil dimuat: {len(df)} baris.\")\n",
        "df.head()\n",
        "\n",
        "# DESKRIPSI DATASET\n",
        "print(\"=\" * 50)\n",
        "print(\"DESKRIPSI DATASET\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Sumber Data      : Twitter (X)\")\n",
        "print(f\"Topik            : Deforestasi & Pengundulan Hutan\")\n",
        "print(f\"Periode          : 20 Nov - 20 Des 2025\")\n",
        "print(f\"Ukuran Dataset   : {len(df)} tweets\")\n",
        "print(f\"Jumlah Kolom     : {len(df.columns)}\")\n",
        "print(f\"Kolom yang ada   : {list(df.columns)}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ltf2PBxwqjX"
      },
      "outputs": [],
      "source": [
        "# Fungsi preprocessing\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Hapus URL\n",
        "    text = re.sub(r'@\\w+', '', text)  # Hapus mention\n",
        "    text = re.sub(r'#\\w+', '', text)  # Hapus hashtag\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Hapus karakter non-alfabet\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalisasi spasi\n",
        "    return text\n",
        "\n",
        "print(f\"Data awal: {len(df)} tweets\")\n",
        "\n",
        "# Terapkan preprocessing\n",
        "df['cleaned_text'] = df['full_text'].apply(clean_text)\n",
        "print(\"Pembersihan teks selesai.\")\n",
        "\n",
        "# Filter tweet dengan minimal 5 kata\n",
        "df = df[df['cleaned_text'].apply(lambda x: len(x.split()) >= 5)]\n",
        "print(f\"Data setelah filter (min 5 kata): {len(df)} tweets\")\n",
        "\n",
        "# Stopwords Indonesia\n",
        "indo_stopwords = set(['yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'dengan', 'untuk',\n",
        "                      'pada', 'adalah', 'dalam', 'tidak', 'akan', 'atau', 'juga', 'ada',\n",
        "                      'bisa', 'lebih', 'sudah', 'saja', 'karena', 'seperti', 'oleh', 'mereka',\n",
        "                      'kita', 'saya', 'anda', 'dia', 'kalau', 'jadi', 'harus', 'lagi', 'pun'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Labeling Sentimen (2 Kelas: Positif/Negatif)\n",
        "\n",
        "**REVISI:** Labeling langsung 2 kelas tanpa kelas Netral dan tanpa confidence score"
      ],
      "metadata": {
        "id": "huX10bBQwqjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gNXdTetwqjY"
      },
      "outputs": [],
      "source": [
        "# LABELING 2 KELAS (TANPA NETRAL, TANPA CONFIDENCE SCORE)\n",
        "\n",
        "import re\n",
        "\n",
        "def label_sentiment(text):\n",
        "    \"\"\"\n",
        "    Labeling dengan pattern matching untuk 2 kelas: Positif dan Negatif.\n",
        "    Tanpa kelas Netral dan tanpa confidence score.\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # POLA POSITIF - Konteks solusi, keberhasilan, dukungan\n",
        "    positive_patterns = [\n",
        "        # Keberhasilan menurunkan/menekan deforestasi\n",
        "        r'deforestasi.{0,30}(turun|berkurang|ditekan|menurun|rendah|nol|berhenti)',\n",
        "        r'(turun|berkurang|ditekan|menurun).{0,30}deforestasi',\n",
        "        r'(nol|zero|tanpa|stop|anti).{0,10}deforestasi',\n",
        "        r'(berhasil|sukses|mampu|bisa).{0,30}(tekan|kurangi|hentikan|cegah).{0,20}deforestasi',\n",
        "\n",
        "        # Reboisasi dan restorasi\n",
        "        r'(reboisasi|restorasi|penghijauan|penanaman pohon)',\n",
        "        r'(tanam|menanam).{0,20}(pohon|bibit|hutan)',\n",
        "        r'(pohon|hutan).{0,20}(ditanam|bertambah|pulih)',\n",
        "        r'(juta|ribu|ribuan).{0,10}(pohon|bibit).{0,10}(ditanam|tanam)',\n",
        "\n",
        "        # Konservasi dan perlindungan\n",
        "        r'(konservasi|pelestarian|perlindungan).{0,20}hutan',\n",
        "        r'(melindungi|menjaga|melestarikan|merawat).{0,20}hutan',\n",
        "        r'(lindungi|jaga|selamatkan|lestarikan).{0,20}hutan',\n",
        "        r'hutan.{0,20}(dilindungi|dijaga|dilestarikan|terjaga)',\n",
        "\n",
        "        # Hasil positif lingkungan\n",
        "        r'(emisi|karbon).{0,20}(turun|berkurang|menurun)',\n",
        "        r'(hijau|lestari|asri).{0,20}(kembali|lagi|tetap)',\n",
        "        r'hutan.{0,20}(pulih|membaik|hijau kembali)',\n",
        "        r'(net gain|forest gain|penambahan hutan)',\n",
        "\n",
        "        # Kebijakan positif\n",
        "        r'(moratorium|larangan).{0,20}(deforestasi|penebangan|pembukaan lahan)',\n",
        "        r'(kebijakan|program|komitmen).{0,20}(anti deforestasi|pro lingkungan|hijau)',\n",
        "\n",
        "        # Ekspresi dukungan untuk lingkungan\n",
        "        r'(setuju|dukung|mendukung).{0,30}(lindungi|jaga|lestarikan|konservasi).{0,20}hutan',\n",
        "        r'(prioritas|utama|penting).{0,20}(melindungi|menjaga|konservasi).{0,20}hutan',\n",
        "\n",
        "        # Sustainable/berkelanjutan\n",
        "        r'(sawit|pertanian|industri).{0,20}(berkelanjutan|sustainable|ramah lingkungan)',\n",
        "        r'(tanpa|bebas).{0,10}deforestasi',\n",
        "\n",
        "        # Apresiasi dan pencapaian\n",
        "        r'(bukti|contoh|teladan).{0,20}(nyata|baik|positif)',\n",
        "        r'(apresiasi|bangga|senang).{0,30}(lingkungan|hutan|konservasi)',\n",
        "    ]\n",
        "\n",
        "    # POLA NEGATIF - Konteks masalah, kerusakan, kritik\n",
        "    negative_patterns = [\n",
        "        # Kerusakan hutan aktif\n",
        "        r'hutan.{0,20}(rusak|hancur|hilang|habis|gundul|musnah|terbakar)',\n",
        "        r'(rusak|hancur|hilang|habis|gundul).{0,20}hutan',\n",
        "        r'(kerusakan|kehancuran|kehilangan).{0,20}hutan',\n",
        "        r'deforestasi.{0,20}(parah|masif|besar|meningkat|meluas)',\n",
        "        r'(laju|tingkat|angka).{0,10}deforestasi.{0,10}(tinggi|naik|meningkat)',\n",
        "\n",
        "        # Penebangan ilegal\n",
        "        r'(ilegal|liar|illegal).{0,20}(logging|penebangan|pembalakan)',\n",
        "        r'(pembalakan|penebangan|pembabatan).{0,20}(liar|ilegal|masif)',\n",
        "        r'(mafia|kartel|sindikat).{0,20}(hutan|kayu|logging)',\n",
        "\n",
        "        # Bencana akibat deforestasi\n",
        "        r'(banjir|longsor|kekeringan|bencana).{0,30}(akibat|karena|dampak|efek).{0,20}(deforestasi|penebangan|gundul)',\n",
        "        r'(deforestasi|penebangan|hutan hilang).{0,30}(sebab|penyebab|akibatkan).{0,20}(banjir|longsor|bencana)',\n",
        "        r'(korban|mengungsi|evakuasi|meninggal).{0,30}(banjir|longsor|bencana)',\n",
        "\n",
        "        # Kebakaran hutan\n",
        "        r'(kebakaran|terbakar|api).{0,20}(hutan|lahan)',\n",
        "        r'(hutan|lahan).{0,20}(terbakar|kebakaran|dilahap api)',\n",
        "        r'(asap|kabut asap|polusi).{0,20}(kebakaran|hutan)',\n",
        "\n",
        "        # Eksploitasi dan perusakan\n",
        "        r'(eksploitasi|mengeruk|menguras|membabat).{0,20}(hutan|alam|sumber daya)',\n",
        "        r'(sawit|tambang|perkebunan).{0,30}(rusak|hancur|habiskan).{0,20}hutan',\n",
        "        r'hutan.{0,20}(dikorbankan|dihabiskan|dibabat).{0,20}(sawit|tambang|bisnis)',\n",
        "\n",
        "        # Kritik kebijakan/pemerintah\n",
        "        r'(pemerintah|rezim|penguasa).{0,30}(gagal|bobrok|korup).{0,20}(lingkungan|hutan)',\n",
        "        r'(izin|konsesi|hgu).{0,30}(bermasalah|ilegal|kontroversial)',\n",
        "        r'(korupsi|suap|kolusi).{0,30}(hutan|lingkungan|kehutanan)',\n",
        "\n",
        "        # Dampak negatif\n",
        "        r'(habitat|ekosistem|satwa).{0,20}(hilang|rusak|hancur|terancam)',\n",
        "        r'(spesies|flora|fauna).{0,20}(punah|terancam|hilang)',\n",
        "        r'(krisis|darurat|ancaman).{0,20}(lingkungan|iklim|ekologi)',\n",
        "\n",
        "        # Ekspresi kemarahan/kritik\n",
        "        r'(miris|tragis|memprihatinkan|mengerikan|parah).{0,30}(hutan|deforestasi|lingkungan)',\n",
        "        r'(stop|hentikan|tolak).{0,20}(deforestasi|penebangan|perusakan)',\n",
        "        r'(jangan|dilarang).{0,20}(rusak|tebang|bakar).{0,20}hutan',\n",
        "\n",
        "        # Kegagalan dan masalah\n",
        "        r'(gagal|tidak berhasil|sia-sia).{0,30}(lindungi|jaga|konservasi)',\n",
        "        r'(masalah|problem|isu).{0,20}(deforestasi|lingkungan|hutan)',\n",
        "    ]\n",
        "\n",
        "    # SCORING DENGAN PATTERN MATCHING\n",
        "    pos_score = 0\n",
        "    neg_score = 0\n",
        "\n",
        "    # Cek pola positif\n",
        "    for pattern in positive_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            pos_score += len(matches)\n",
        "\n",
        "    # Cek pola negatif\n",
        "    for pattern in negative_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            neg_score += len(matches)\n",
        "\n",
        "    # KEYWORD FALLBACK (jika tidak ada pattern yang cocok)\n",
        "    if pos_score == 0 and neg_score == 0:\n",
        "        # Gunakan keyword sederhana sebagai fallback\n",
        "        simple_neg = ['rusak', 'hancur', 'banjir', 'longsor', 'ilegal', 'gagal', 'parah',\n",
        "                      'korban', 'bencana', 'terbakar', 'gundul', 'habis', 'musnah']\n",
        "        simple_pos = ['reboisasi', 'konservasi', 'lindungi', 'tanam pohon', 'lestari',\n",
        "                      'berkelanjutan', 'berhasil', 'pulih', 'hijau']\n",
        "\n",
        "        for word in simple_neg:\n",
        "            if word in text:\n",
        "                neg_score += 0.5\n",
        "\n",
        "        for word in simple_pos:\n",
        "            if word in text:\n",
        "                pos_score += 0.5\n",
        "\n",
        "    # TENTUKAN LABEL\n",
        "\n",
        "    if pos_score > neg_score:\n",
        "        return 'Positif'\n",
        "    else:\n",
        "        return 'Negatif'\n",
        "\n",
        "\n",
        "# TERAPKAN LABELING\n",
        "df['label'] = df['cleaned_text'].apply(label_sentiment)\n",
        "\n",
        "# ANALISIS HASIL\n",
        "print(\"=\" * 60)\n",
        "print(\"HASIL LABELING SENTIMEN (2 KELAS)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n DISTRIBUSI LABEL:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nPersentase:\")\n",
        "print((df['label'].value_counts(normalize=True) * 100).round(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz3CB4nKwqjb"
      },
      "outputs": [],
      "source": [
        "# SAMPLE HASIL LABELING\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAMPLE HASIL LABELING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n SAMPLE POSITIF:\")\n",
        "for idx, row in df[df['label'] == 'Positif'].head(5).iterrows():\n",
        "    print(f\"‚Ä¢ {row['cleaned_text'][:100]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n SAMPLE NEGATIF:\")\n",
        "for idx, row in df[df['label'] == 'Negatif'].head(5).iterrows():\n",
        "    print(f\"‚Ä¢ {row['cleaned_text'][:100]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvygxxDkwqjc"
      },
      "outputs": [],
      "source": [
        "# Simpan dataset yang sudah dilabel\n",
        "df.to_csv('tweets_labeled.csv', index=False)\n",
        "print(f\"Dataset tersimpan: {len(df)} tweets dengan label Positif/Negatif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualisasi Distribusi Label"
      ],
      "metadata": {
        "id": "sVliMB2Awqjd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1FjN3i5wqjd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_counts = df['label'].value_counts()\n",
        "n_labels = len(label_counts)\n",
        "\n",
        "colors_dict = {'Negatif': '#e74c3c', 'Positif': '#2ecc71'}\n",
        "color_list = [colors_dict[label] for label in label_counts.index]\n",
        "explode_tuple = tuple([0.05] * n_labels)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Pie chart\n",
        "axes[0].pie(label_counts,\n",
        "            labels=label_counts.index,\n",
        "            autopct='%1.1f%%',\n",
        "            startangle=140,\n",
        "            colors=color_list,\n",
        "            explode=explode_tuple)\n",
        "axes[0].set_title('Distribusi Sentiment\\nTweet Deforestasi (2 Kelas)', fontweight='bold', fontsize=12)\n",
        "\n",
        "# Bar chart\n",
        "bars = axes[1].bar(label_counts.index,\n",
        "                   label_counts.values,\n",
        "                   color=color_list)\n",
        "axes[1].set_title('Jumlah Tweet per Label', fontweight='bold', fontsize=12)\n",
        "axes[1].set_xlabel('Label')\n",
        "axes[1].set_ylabel('Jumlah')\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                 f'{int(height)}',\n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTIzd0NXwqje"
      },
      "outputs": [],
      "source": [
        "# VERIFIKASI SAMPLE PER LABEL\n",
        "print(\"=== SAMPLE TWEET YANG DILABEL POSITIF ===\\n\")\n",
        "samples_pos = df[df['label'] == 'Positif']['cleaned_text'].head(10).tolist()\n",
        "for i, text in enumerate(samples_pos, 1):\n",
        "    print(f\"{i}. {text[:100]}...\")\n",
        "\n",
        "print(\"\\n=== SAMPLE TWEET YANG DILABEL NEGATIF ===\\n\")\n",
        "samples_neg = df[df['label'] == 'Negatif']['cleaned_text'].head(10).tolist()\n",
        "for i, text in enumerate(samples_neg, 1):\n",
        "    print(f\"{i}. {text[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BHTI-z_wqjf"
      },
      "outputs": [],
      "source": [
        "# WordCloud per sentiment\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "for label in df['label'].unique():\n",
        "    text = ' '.join(df[df['label']==label]['cleaned_text'])\n",
        "    wordcloud = WordCloud(stopwords=indo_stopwords, width=800, height=400).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'WordCloud - {label}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Split Data Train/Validation/Test"
      ],
      "metadata": {
        "id": "SHdRAUshwqjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5wwjO-Swqjh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0uTj66mwqjh"
      },
      "outputs": [],
      "source": [
        "# ANALISIS & HANDLING DATA IMBALANCE\n",
        "\n",
        "print(\"Distribusi Label pada Data Training:\")\n",
        "print(train_df['label'].value_counts())\n",
        "print(f\"\\nPersentase:\")\n",
        "print((train_df['label'].value_counts(normalize=True) * 100).round(1))\n",
        "\n",
        "# Visualisasi distribusi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for ax, (name, data) in zip(axes, [('Train', train_df), ('Validation', val_df), ('Test', test_df)]):\n",
        "    data['label'].value_counts().plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c'])\n",
        "    ax.set_title(f'Distribusi {name} Set')\n",
        "    ax.set_xlabel('Label')\n",
        "    ax.set_ylabel('Jumlah')\n",
        "    ax.tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Setup Model IndoBERT\n",
        "\n"
      ],
      "metadata": {
        "id": "_AKJyjaKwqji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKW5Sm-Jwqji"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate evaluate scikit-learn\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# REVISI: Menggunakan IndoBERT untuk hyperparameter tuning (sama dengan model comparison)\n",
        "model_name = \"indobenchmark/indobert-base-p1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Mapping label ke angka (2 kelas)\n",
        "label2id = {'Negatif': 0, 'Positif': 1}\n",
        "id2label = {0: 'Negatif', 1: 'Positif'}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# HANDLING IMBALANCE DENGAN CLASS WEIGHTS\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "train_labels = [label2id[l] for l in train_df['label']]\n",
        "existing_classes = np.unique(train_labels)\n",
        "\n",
        "# Hitung bobot hanya untuk kelas yang tersedia\n",
        "weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=existing_classes,\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "class_weights = torch.zeros(2, dtype=torch.float)\n",
        "for i, cls_idx in enumerate(existing_classes):\n",
        "    class_weights[cls_idx] = float(weights[i])\n",
        "\n",
        "print(\"--- Hasil Perhitungan Class Weights ---\")\n",
        "for i, label in id2label.items():\n",
        "    status = \"Tersedia\" if i in existing_classes else \"KOSONG\"\n",
        "    print(f\"  {label} ({i}): {class_weights[i]:.3f} - {status}\")\n",
        "\n",
        "# CUSTOM TRAINER\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        weight = self.class_weights.to(logits.device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0feFfDxGwqjj"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}))\n",
        "val_dataset = Dataset.from_pandas(val_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}))\n",
        "test_dataset = Dataset.from_pandas(test_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}))\n",
        "\n",
        "# Encode labels\n",
        "train_dataset = train_dataset.map(lambda x: {'label': label2id[x['label']]})\n",
        "val_dataset = val_dataset.map(lambda x: {'label': label2id[x['label']]})\n",
        "test_dataset = test_dataset.map(lambda x: {'label': label2id[x['label']]})\n",
        "\n",
        "# Tokenize\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Hyperparameter Tuning dengan IndoBERT\n",
        "\n"
      ],
      "metadata": {
        "id": "TnRhj6C2wqjk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trQz2DcUwqjk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "\n",
        "# Define metrics\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Hyperparameter yang akan diuji\n",
        "learning_rates = [1e-5, 2e-5, 5e-5]\n",
        "batch_sizes = [8, 16]\n",
        "epochs_list = [3, 5]\n",
        "\n",
        "# REVISI: Model yang digunakan untuk hyperparameter tuning = IndoBERT (sama dengan model comparison)\n",
        "hp_model_name = \"indobenchmark/indobert-base-p1\"\n",
        "\n",
        "experiment_results = []\n",
        "experiment_id = 1\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"HYPERPARAMETER TUNING DENGAN MODEL: {hp_model_name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Loop untuk setiap kombinasi\n",
        "for lr in learning_rates:\n",
        "    for batch in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Eksperimen {experiment_id}: LR={lr}, Batch={batch}, Epochs={epochs}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Load ulang model untuk setiap eksperimen (PENTING!)\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                hp_model_name,\n",
        "                num_labels=2,\n",
        "                id2label=id2label,\n",
        "                label2id=label2id\n",
        "            )\n",
        "\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=f\"./results/exp_{experiment_id}\",\n",
        "                eval_strategy=\"epoch\",\n",
        "                save_strategy=\"epoch\",\n",
        "                learning_rate=lr,\n",
        "                per_device_train_batch_size=batch,\n",
        "                per_device_eval_batch_size=batch,\n",
        "                num_train_epochs=epochs,\n",
        "                weight_decay=0.01,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"accuracy\",\n",
        "                report_to=\"none\",\n",
        "                save_total_limit=1,\n",
        "            )\n",
        "\n",
        "            # Trainer\n",
        "            trainer = WeightedTrainer(\n",
        "                class_weights=class_weights,\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                compute_metrics=compute_metrics,\n",
        "            )\n",
        "\n",
        "            # Train!\n",
        "            train_result = trainer.train()\n",
        "\n",
        "            # Evaluasi\n",
        "            eval_result = trainer.evaluate()\n",
        "\n",
        "            # Simpan hasil\n",
        "            experiment_results.append({\n",
        "                'experiment_id': experiment_id,\n",
        "                'learning_rate': lr,\n",
        "                'batch_size': batch,\n",
        "                'epochs': epochs,\n",
        "                'train_loss': train_result.training_loss,\n",
        "                'val_accuracy': eval_result['eval_accuracy'],\n",
        "                'val_loss': eval_result['eval_loss'],\n",
        "            })\n",
        "\n",
        "            print(f\" Val Accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
        "\n",
        "            # Bersihkan memory\n",
        "            del model, trainer\n",
        "            import gc\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            experiment_id += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SEMUA EKSPERIMEN SELESAI!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pZHY2MGwqjm"
      },
      "outputs": [],
      "source": [
        "# Hasil semua eksperimen\n",
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(experiment_results)\n",
        "results_df = results_df.sort_values('val_accuracy', ascending=False)\n",
        "\n",
        "print(\"=== HASIL HYPERPARAMETER TUNING ===\\n\")\n",
        "display(results_df)\n",
        "\n",
        "# Best configuration\n",
        "best = results_df.iloc[0]\n",
        "print(f\"\\nüèÜ BEST CONFIGURATION:\")\n",
        "print(f\"   Learning Rate: {best['learning_rate']}\")\n",
        "print(f\"   Batch Size: {best['batch_size']}\")\n",
        "print(f\"   Epochs: {best['epochs']}\")\n",
        "print(f\"   Val Accuracy: {best['val_accuracy']:.4f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "fig.suptitle('Hasil Hyperparameter Tuning (IndoBERT)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 1. Accuracy by Learning Rate\n",
        "ax1 = axes[0]\n",
        "for batch in batch_sizes:\n",
        "    data = results_df[results_df['batch_size'] == batch]\n",
        "    data_grouped = data.groupby('learning_rate')['val_accuracy'].mean()\n",
        "    ax1.plot(data_grouped.index, data_grouped.values, marker='o', label=f'Batch {batch}')\n",
        "ax1.set_xlabel('Learning Rate')\n",
        "ax1.set_ylabel('Val Accuracy')\n",
        "ax1.set_xscale('log')\n",
        "ax1.legend()\n",
        "ax1.set_title('Accuracy vs Learning Rate')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Accuracy by Batch Size\n",
        "ax2 = axes[1]\n",
        "batch_acc = results_df.groupby('batch_size')['val_accuracy'].mean()\n",
        "ax2.bar(batch_acc.index.astype(str), batch_acc.values, color=['#2E86AB', '#A23B72'])\n",
        "ax2.set_xlabel('Batch Size')\n",
        "ax2.set_ylabel('Val Accuracy')\n",
        "ax2.set_title('Accuracy vs Batch Size')\n",
        "\n",
        "# 3. Accuracy by Epochs\n",
        "ax3 = axes[2]\n",
        "epoch_acc = results_df.groupby('epochs')['val_accuracy'].mean()\n",
        "ax3.bar(epoch_acc.index.astype(str), epoch_acc.values, color=['#06A77D', '#F18F01'])\n",
        "ax3.set_xlabel('Epochs')\n",
        "ax3.set_ylabel('Val Accuracy')\n",
        "ax3.set_title('Accuracy vs Epochs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('hyperparameter_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Generate tabel markdown\n",
        "print(\"\\n### Tabel Hasil Hyperparameter Tuning\\n\")\n",
        "print(\"| No | Learning Rate | Batch Size | Epochs | Val Accuracy | Val Loss |\")\n",
        "print(\"|:--:|:-------------:|:----------:|:------:|:------------:|:--------:|\")\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"| {row['experiment_id']} | {row['learning_rate']} | {int(row['batch_size'])} | \"\n",
        "          f\"{int(row['epochs'])} | {row['val_accuracy']:.4f} | {row['val_loss']:.4f} |\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Perbandingan Model\n",
        "\n",
        "Menggunakan hyperparameter terbaik dari hasil tuning"
      ],
      "metadata": {
        "id": "M84Vo76bwqjo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUzrhPp3wqjp"
      },
      "outputs": [],
      "source": [
        "# EKSPERIMEN: PERBANDINGAN MODEL\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Ambil best hyperparameter dari eksperimen sebelumnya\n",
        "best = results_df.iloc[0]\n",
        "BEST_LR = best['learning_rate']\n",
        "BEST_BATCH = int(best['batch_size'])\n",
        "BEST_EPOCHS = int(best['epochs'])\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EKSPERIMEN PERBANDINGAN MODEL\")\n",
        "print(f\"Menggunakan: LR={BEST_LR}, Batch={BEST_BATCH}, Epochs={BEST_EPOCHS}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Daftar model yang akan dibandingkan\n",
        "MODELS = [\n",
        "    {\n",
        "        'name': 'IndoLEM-IndoBERT',\n",
        "        'model_id': 'indolem/indobert-base-uncased'\n",
        "    },\n",
        "    {\n",
        "        'name': 'IndoBenchmark-IndoBERT',\n",
        "        'model_id': 'indobenchmark/indobert-base-p1'\n",
        "    },\n",
        "    {\n",
        "        'name': 'mBERT-Multilingual',\n",
        "        'model_id': 'bert-base-multilingual-uncased'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Simpan hasil\n",
        "model_results = []\n",
        "\n",
        "for model_info in MODELS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {model_info['name']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Cleanup memory sebelum load model baru\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_info['model_id'])\n",
        "\n",
        "    train_ds = Dataset.from_pandas(\n",
        "        train_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}).reset_index(drop=True)\n",
        "    )\n",
        "    val_ds = Dataset.from_pandas(\n",
        "        val_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}).reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['text'],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "    train_ds = train_ds.map(lambda x: {'label': label2id[x['label']]})\n",
        "    val_ds = val_ds.map(lambda x: {'label': label2id[x['label']]})\n",
        "\n",
        "    train_ds = train_ds.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "    val_ds = val_ds.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "    train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    val_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_info['model_id'],\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{model_info['name'].replace(' ', '_')}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=BEST_LR,\n",
        "        per_device_train_batch_size=BEST_BATCH,\n",
        "        per_device_eval_batch_size=BEST_BATCH,\n",
        "        num_train_epochs=BEST_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        report_to=\"none\",\n",
        "        save_total_limit=1,\n",
        "    )\n",
        "\n",
        "    # Trainer dengan dataset yang sudah di-tokenize\n",
        "    trainer = WeightedTrainer(\n",
        "        class_weights=class_weights,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    train_result = trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "\n",
        "    # Simpan hasil\n",
        "    model_results.append({\n",
        "        'model_name': model_info['name'],\n",
        "        'model_id': model_info['model_id'],\n",
        "        'val_accuracy': eval_result['eval_accuracy'],\n",
        "        'val_loss': eval_result['eval_loss'],\n",
        "        'train_loss': train_result.training_loss,\n",
        "    })\n",
        "\n",
        "    print(f\" {model_info['name']} - Val Accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
        "\n",
        "    # Cleanup setelah selesai\n",
        "    del model, trainer, tokenizer, train_ds, val_ds\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERBANDINGAN MODEL SELESAI!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_results_df = pd.DataFrame(model_results).sort_values('val_accuracy', ascending=False)\n",
        "display(model_results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6RvYOpDwqjs"
      },
      "outputs": [],
      "source": [
        "# Visualisasi perbandingan model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 1. Accuracy comparison\n",
        "ax1 = axes[0]\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
        "bars = ax1.bar(model_results_df['model_name'], model_results_df['val_accuracy'], color=colors)\n",
        "ax1.set_xlabel('Model')\n",
        "ax1.set_ylabel('Validation Accuracy')\n",
        "ax1.set_title('Perbandingan Accuracy antar Model', fontweight='bold')\n",
        "ax1.set_ylim(0.5, 1.0)\n",
        "for bar, acc in zip(bars, model_results_df['val_accuracy']):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
        "             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "ax1.tick_params(axis='x', rotation=15)\n",
        "\n",
        "# 2. Loss comparison\n",
        "ax2 = axes[1]\n",
        "bars = ax2.bar(model_results_df['model_name'], model_results_df['val_loss'], color=colors)\n",
        "ax2.set_xlabel('Model')\n",
        "ax2.set_ylabel('Validation Loss')\n",
        "ax2.set_title('Perbandingan Loss antar Model', fontweight='bold')\n",
        "for bar, loss in zip(bars, model_results_df['val_loss']):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
        "             f'{loss:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "ax2.tick_params(axis='x', rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Best model\n",
        "best_model = model_results_df.iloc[0]\n",
        "print(f\"\\nüèÜ MODEL TERBAIK: {best_model['model_name']}\")\n",
        "print(f\"   Accuracy: {best_model['val_accuracy']:.4f}\")\n",
        "print(f\"   Loss: {best_model['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Training Model Terbaik & Evaluasi Final"
      ],
      "metadata": {
        "id": "k2i9gwTHwqjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuBUmPS1wqju"
      },
      "outputs": [],
      "source": [
        "# Training model terbaik untuk evaluasi final\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Gunakan model terbaik\n",
        "best_model_id = model_results_df.iloc[0]['model_id']\n",
        "best_model_name = model_results_df.iloc[0]['model_name']\n",
        "\n",
        "print(f\"Training final model: {best_model_name}\")\n",
        "\n",
        "# Load tokenizer dan model\n",
        "final_tokenizer = AutoTokenizer.from_pretrained(best_model_id)\n",
        "\n",
        "# Prepare datasets\n",
        "train_ds = Dataset.from_pandas(\n",
        "    train_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}).reset_index(drop=True)\n",
        ")\n",
        "test_ds = Dataset.from_pandas(\n",
        "    test_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}).reset_index(drop=True)\n",
        ")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return final_tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.map(lambda x: {'label': label2id[x['label']]})\n",
        "test_ds = test_ds.map(lambda x: {'label': label2id[x['label']]})\n",
        "\n",
        "train_ds = train_ds.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "test_ds = test_ds.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# Load model\n",
        "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    best_model_id,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/final_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=BEST_LR,\n",
        "    per_device_train_batch_size=BEST_BATCH,\n",
        "    per_device_eval_batch_size=BEST_BATCH,\n",
        "    num_train_epochs=BEST_EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "final_trainer = WeightedTrainer(\n",
        "    class_weights=class_weights,\n",
        "    model=final_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "final_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNqNFTt6wqjw"
      },
      "outputs": [],
      "source": [
        "# Evaluasi pada test set\n",
        "predictions = final_trainer.predict(test_ds)\n",
        "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "# Classification report\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUASI FINAL PADA TEST SET\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=['Negatif', 'Positif']))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negatif', 'Positif'],\n",
        "            yticklabels=['Negatif', 'Positif'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontweight='bold')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Test accuracy\n",
        "test_accuracy = (pred_labels == true_labels).mean()\n",
        "print(f\"\\n TEST ACCURACY: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFQS1U4iwqjw"
      },
      "outputs": [],
      "source": [
        "# Simpan model\n",
        "final_model.save_pretrained('./final_sentiment_model')\n",
        "final_tokenizer.save_pretrained('./final_sentiment_model')\n",
        "print(\"Model tersimpan di ./final_sentiment_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Ringkasan Hasil\n"
      ],
      "metadata": {
        "id": "gfG4aLd8wqjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ILFlB-wqjy"
      },
      "outputs": [],
      "source": [
        "# RINGKASAN AKHIR\n",
        "print(\"=\"*60)\n",
        "print(\"RINGKASAN HASIL ANALISIS SENTIMEN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n DATASET:\")\n",
        "print(f\"   - Total data setelah preprocessing: {len(df)} tweets\")\n",
        "print(f\"   - Kelas: 2 (Positif, Negatif)\")\n",
        "print(f\"   - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "print(f\"\\n HYPERPARAMETER TERBAIK:\")\n",
        "print(f\"   - Learning Rate: {BEST_LR}\")\n",
        "print(f\"   - Batch Size: {BEST_BATCH}\")\n",
        "print(f\"   - Epochs: {BEST_EPOCHS}\")\n",
        "\n",
        "print(f\"\\n MODEL TERBAIK: {best_model_name}\")\n",
        "print(f\"   - Validation Accuracy: {model_results_df.iloc[0]['val_accuracy']:.4f}\")\n",
        "print(f\"   - Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}